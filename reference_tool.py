# -*- coding: utf-8 -*-
import os
import json
import sys
import numpy as np
from pathlib import Path
from dotenv import load_dotenv  # å¯¼å…¥dotenvåŠ è½½.envé…ç½®

# ====================== 1. åŠ è½½.envé…ç½®ï¼ˆè½¯ç¼–ç æ ¸å¿ƒï¼‰ ======================
# åŠ è½½.envæ–‡ä»¶ï¼ˆä¼˜å…ˆä»è„šæœ¬æ‰€åœ¨ç›®å½•æ‰¾ï¼Œæ‰¾ä¸åˆ°åˆ™ä»é¡¹ç›®æ ¹ç›®å½•æ‰¾ï¼‰
load_dotenv()

# ã€ä¿®å¤ã€‘å¼ºåˆ¶æŒ‡å®šBASE_DIRä¸ºDeepfakedetectionAgent2ï¼ˆå’Œä¸»ç¨‹åºä¸€è‡´ï¼‰ï¼Œé¿å…è·¯å¾„æ¨å¯¼é”™è¯¯
BASE_DIR = os.getenv("BASE_DIR")  # å…œåº•å€¼æ”¹ä¸ºä¸»ç¨‹åºçš„æ ¹ç›®å½•

# ====================== å…¨å±€é…ç½®ï¼ˆåŸºäºBASE_DIRè½¯ç¼–ç ï¼Œè·¨å¹³å°å…¼å®¹ï¼‰ ======================
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# å„Agentè¾“å‡ºç›®å½•ï¼ˆåŸºäºBASE_DIRåŠ¨æ€æ‹¼æ¥ï¼Œè½¯ç¼–ç ï¼‰
SUSPICIOUS_FEATURE_ROOT = os.path.join(BASE_DIR, "outputs", "suspicious_features")
ASR_OUTPUT_ROOT = os.path.join(BASE_DIR, "outputs", "asr")
REFERENCE_OUTPUT_ROOT = os.path.join(BASE_DIR, "outputs", "reference_report")
os.makedirs(REFERENCE_OUTPUT_ROOT, exist_ok=True)

# ====================== æ ¸å¿ƒï¼šåŸºäºLibriSpeechç»Ÿè®¡çš„å¼‚å¸¸é˜ˆå€¼ ======================
ANOMALY_THRESHOLDS = {
    # MFCCé˜ˆå€¼ï¼ˆæ¥è‡ªLibriSpeech 500æ ·æœ¬ç»Ÿè®¡ï¼‰
    "mfcc_mean_abs": 0.5,             # MFCCå‡å€¼ç»å¯¹å€¼å¼‚å¸¸é˜ˆå€¼
    "mfcc_std_upper": 35.0141,        # MFCCæ•´ä½“æ ‡å‡†å·®å¼‚å¸¸ä¸Šé™
    "mfcc_inner_std_upper": 44.6855,  # MFCCç»´åº¦å†…æ ‡å‡†å·®å¼‚å¸¸ä¸Šé™
    # æ¢…å°”é¢‘è°±èƒ½é‡é˜ˆå€¼ï¼ˆæ¥è‡ªLibriSpeech 500æ ·æœ¬ç»Ÿè®¡ï¼‰
    "mel_energy_upper": -43.5002,     # æ¢…å°”èƒ½é‡åé«˜å¼‚å¸¸é˜ˆå€¼
    "mel_energy_lower": -65.9447,     # æ¢…å°”èƒ½é‡åä½å¼‚å¸¸é˜ˆå€¼
    # é£é™©ç­‰çº§åˆ¤å®šé˜ˆå€¼
    "high_risk_anomaly_count": 2,     # â‰¥2ä¸ªå¼‚å¸¸ç±»å‹ â†’ é«˜é£é™©
    "medium_risk_anomaly_count": 1    # 1ä¸ªå¼‚å¸¸ç±»å‹ â†’ ä¸­é£é™©
}

# ====================== å·¥å…·å‡½æ•°ï¼šè¯»å–ASR+è¯´è¯äººæ•°æ® ======================
def load_asr_diarization_data(audio_filename):
    """
    è¯»å–æŒ‡å®šéŸ³é¢‘çš„ASR+è¯´è¯äººåˆ†å‰²ç»“æœ
    :param audio_filename: è¯­éŸ³æ–‡ä»¶åï¼ˆå»åç¼€ï¼‰ï¼Œå¦‚LA_E_1000147
    :return: ASRæ•°æ®å­—å…¸ / Noneï¼ˆæ— æ•°æ®æ—¶ï¼‰
    """
    asr_json_path = os.path.join(ASR_OUTPUT_ROOT, f"{audio_filename}_asr_diarization.json")
    
    # æ£€æŸ¥ASRæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(asr_json_path):
        print(f"â„¹ï¸ æœªæ‰¾åˆ°{audio_filename}çš„ASR+è¯´è¯äººæ•°æ®ï¼š{asr_json_path}")
        return None
    
    try:
        with open(asr_json_path, "r", encoding="utf-8") as f:
            asr_data = json.load(f)
        
        # æ ¡éªŒASRæ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if not asr_data.get("success"):
            print(f"â„¹ï¸ {audio_filename}çš„ASRæ•°æ®æ— æ•ˆï¼š{asr_data.get('error')}")
            return None
        
        return asr_data
    except Exception as e:
        print(f"â„¹ï¸ è§£æASRæ•°æ®å¤±è´¥ï¼š{str(e)}")
        return None

# ====================== å·¥å…·å‡½æ•°ï¼šåŒ¹é…å¯ç–‘ç‰‡æ®µä¸è¯­éŸ³å†…å®¹ ======================
def match_suspicious_segment_with_text(suspicious_segment, asr_segments):
    """
    åŒ¹é…å¯ç–‘ç‰‡æ®µå¯¹åº”çš„è¯­éŸ³å†…å®¹
    :param suspicious_segment: å¯ç–‘ç‰‡æ®µï¼ˆstart/endï¼‰
    :param asr_segments: ASRè¯çº§åˆ†æ®µæ•°æ®
    :return: è¯¥æ—¶é—´æ®µå†…çš„è¯­éŸ³å†…å®¹åˆ—è¡¨
    """
    seg_start = suspicious_segment["start"]
    seg_end = suspicious_segment["end"]
    matched_words = []
    
    for word_seg in asr_segments:
        # è¯çš„æ—¶é—´èŒƒå›´ä¸å¯ç–‘ç‰‡æ®µæœ‰äº¤é›†å³åŒ¹é…
        word_start = word_seg["start"]
        word_end = word_seg["end"]
        
        if not (word_end < seg_start or word_start > seg_end):
            matched_words.append({
                "speaker_id": word_seg["speaker_id"],
                "word": word_seg["word"],
                "start": word_seg["start"],
                "end": word_seg["end"]
            })
    
    # æ‹¼æ¥æˆå®Œæ•´æ–‡æœ¬
    matched_text = " ".join([w["word"] for w in matched_words])
    return {
        "matched_words": matched_words,
        "matched_text": matched_text,
        "total_matched_words": len(matched_words),
        "speakers_in_segment": list(set([w["speaker_id"] for w in matched_words]))
    }

# ====================== 1. è¯»å–å¯ç–‘ç‰‡æ®µç‰¹å¾æ±‡æ€»æ–‡ä»¶ ======================
def load_suspicious_features(audio_filename):
    """
    è¯»å–æŒ‡å®šè¯­éŸ³æ–‡ä»¶çš„å¯ç–‘ç‰‡æ®µç‰¹å¾æ±‡æ€»æ–‡ä»¶
    """
    summary_path = os.path.join(SUSPICIOUS_FEATURE_ROOT, audio_filename, "suspicious_features_summary.json")
    
    if not os.path.exists(summary_path):
        return {
            "success": False,
            "error": f"å¯ç–‘ç‰‡æ®µç‰¹å¾æ±‡æ€»æ–‡ä»¶ä¸å­˜åœ¨ï¼š{summary_path}"
        }
    
    try:
        with open(summary_path, "r", encoding="utf-8") as f:
            feature_data = json.load(f)
        
        if not feature_data.get("success"):
            return {
                "success": False,
                "error": f"ç‰¹å¾æå–å¤±è´¥ï¼Œæ±‡æ€»æ–‡ä»¶æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€"
            }
        
        return {
            "success": True,
            "data": feature_data
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æç‰¹å¾æ–‡ä»¶å¤±è´¥ï¼š{str(e)}"
        }

# ====================== 2. ç‰¹å¾å¼‚å¸¸åˆ†ææ ¸å¿ƒé€»è¾‘ï¼ˆæ ¸å¿ƒæ›´æ–°ï¼‰ ======================
def analyze_feature_anomaly(segment_feature, asr_data=None):
    """
    åˆ†æå•ä¸ªå¯ç–‘ç‰‡æ®µçš„ç‰¹å¾å¼‚å¸¸ + å…³è”è¯­éŸ³å†…å®¹/è¯´è¯äºº
    æ ¸å¿ƒæ›´æ–°ï¼šåŸºäºLibriSpeechç»Ÿè®¡é˜ˆå€¼åˆ¤å®šå¼‚å¸¸
    """
    analysis_lines = []
    segment_id = segment_feature["segment_id"]
    time_range = segment_feature["time_range"]
    time_str = f"{time_range['start']}s - {time_range['end']}s"

    # åŸºç¡€ä¿¡æ¯
    analysis_lines.append(f"### ç‰‡æ®µ{segment_id}ï¼ˆæ—¶é—´èŒƒå›´ï¼š{time_str}ï¼‰")

    # ===== ASR+è¯´è¯äººå†…å®¹å…³è” =====
    if asr_data and asr_data.get("segments"):
        match_result = match_suspicious_segment_with_text(time_range, asr_data["segments"])
        analysis_lines.append(f"- **è¯­éŸ³å†…å®¹**ï¼š{match_result['matched_text'] or 'æ— åŒ¹é…å†…å®¹'}")
        analysis_lines.append(f"- **è¯´è¯äºº**ï¼š{', '.join(match_result['speakers_in_segment']) or 'UNKNOWN'}")
        analysis_lines.append(f"- **åŒ¹é…è¯æ•°**ï¼š{match_result['total_matched_words']}")
    else:
        analysis_lines.append(f"- **è¯­éŸ³å†…å®¹**ï¼šæœªè·å–åˆ°ASRæ•°æ®")
        analysis_lines.append(f"- **è¯´è¯äºº**ï¼šæœªè·å–åˆ°è¯´è¯äººæ•°æ®")
    
    analysis_lines.append("")  # ç©ºè¡Œåˆ†éš”

    # ===== æ ¸å¿ƒæ›´æ–°ï¼šMFCCç‰¹å¾åˆ†æï¼ˆåŸºäºLibriSpeechç»Ÿè®¡é˜ˆå€¼ï¼‰ =====
    mfcc_feature = segment_feature["mfcc_feature"]
    if mfcc_feature["success"]:
        mfcc_stats = mfcc_feature["mfcc_stats"]
        mfcc_mean_abs = abs(mfcc_stats["mean"])  # å‡å€¼ç»å¯¹å€¼
        mfcc_std = mfcc_stats["std"]              # å¯¹åº”mfcc_std_meanï¼ˆæ•´ä½“æ ‡å‡†å·®ï¼‰
        
        mfcc_analysis = []
        # 1. MFCCå‡å€¼å¼‚å¸¸åˆ¤å®š
        if mfcc_mean_abs > ANOMALY_THRESHOLDS["mfcc_mean_abs"]:
            mfcc_analysis.append(f"MFCCå‡å€¼ç»å¯¹å€¼({round(mfcc_mean_abs, 3)})è¶…å‡ºæ­£å¸¸èŒƒå›´ï¼ˆâ‰¤{ANOMALY_THRESHOLDS['mfcc_mean_abs']}ï¼‰")
        # 2. MFCCæ•´ä½“æ ‡å‡†å·®å¼‚å¸¸åˆ¤å®šï¼ˆæ ¸å¿ƒï¼‰
        if mfcc_std > ANOMALY_THRESHOLDS["mfcc_std_upper"]:
            mfcc_analysis.append(f"MFCCæ•´ä½“æ ‡å‡†å·®({round(mfcc_std, 3)})è¶…å‡ºçœŸäººè¯­éŸ³åŸºå‡†ï¼ˆâ‰¤{ANOMALY_THRESHOLDS['mfcc_std_upper']}ï¼‰ï¼Œé¢‘è°±æ³¢åŠ¨å¼‚å¸¸ï¼ˆåˆæˆéŸ³é¢‘å…¸å‹ç‰¹å¾ï¼‰")
        
        if mfcc_analysis:
            analysis_lines.append(f"- **MFCCç‰¹å¾å¼‚å¸¸**ï¼š{'; '.join(mfcc_analysis)}ï¼›")
        else:
            analysis_lines.append(f"- **MFCCç‰¹å¾**ï¼šå‡å€¼ç»å¯¹å€¼({round(mfcc_mean_abs, 3)})ã€æ•´ä½“æ ‡å‡†å·®({round(mfcc_std, 3)})å‡ç¬¦åˆçœŸäººè¯­éŸ³åŸºå‡†ï¼›")
    else:
        analysis_lines.append(f"- **MFCCç‰¹å¾**ï¼šæå–å¤±è´¥ â†’ {mfcc_feature.get('error', 'æœªçŸ¥é”™è¯¯')}ï¼›")

    # ===== æ ¸å¿ƒæ›´æ–°ï¼šæ¢…å°”é¢‘è°±ç‰¹å¾åˆ†æï¼ˆåŸºäºLibriSpeechç»Ÿè®¡é˜ˆå€¼ï¼‰ =====
    mel_feature = segment_feature["mel_feature"]
    if mel_feature["success"]:
        mel_stats = mel_feature["mel_energy_stats"]
        mel_mean = mel_stats["mean"]  # æ¢…å°”èƒ½é‡å‡å€¼ï¼ˆdBï¼‰
        
        mel_analysis = []
        # 1. æ¢…å°”èƒ½é‡åé«˜å¼‚å¸¸
        if mel_mean > ANOMALY_THRESHOLDS["mel_energy_upper"]:
            mel_analysis.append(f"æ¢…å°”èƒ½é‡å‡å€¼({round(mel_mean, 1)}dB)åé«˜ï¼ˆæ­£å¸¸â‰¤{ANOMALY_THRESHOLDS['mel_energy_upper']}dBï¼‰ï¼Œé¢‘åŸŸèƒ½é‡åˆ†å¸ƒå¼‚å¸¸")
        # 2. æ¢…å°”èƒ½é‡åä½å¼‚å¸¸
        elif mel_mean < ANOMALY_THRESHOLDS["mel_energy_lower"]:
            mel_analysis.append(f"æ¢…å°”èƒ½é‡å‡å€¼({round(mel_mean, 1)}dB)åä½ï¼ˆæ­£å¸¸â‰¥{ANOMALY_THRESHOLDS['mel_energy_lower']}dBï¼‰ï¼Œé«˜é¢‘ä¿¡æ¯ç¼ºå¤±ï¼ˆåˆæˆéŸ³é¢‘å…¸å‹ç‰¹å¾ï¼‰")
        
        if mel_analysis:
            analysis_lines.append(f"- **æ¢…å°”é¢‘è°±ç‰¹å¾å¼‚å¸¸**ï¼š{'; '.join(mel_analysis)}ï¼›")
        else:
            analysis_lines.append(f"- **æ¢…å°”é¢‘è°±ç‰¹å¾**ï¼šèƒ½é‡å‡å€¼({round(mel_mean, 1)}dB)ç¬¦åˆçœŸäººè¯­éŸ³åŸºå‡†ï¼ˆ{ANOMALY_THRESHOLDS['mel_energy_lower']}~{ANOMALY_THRESHOLDS['mel_energy_upper']}dBï¼‰ï¼›")
    else:
        analysis_lines.append(f"- **æ¢…å°”é¢‘è°±ç‰¹å¾**ï¼šæå–å¤±è´¥ â†’ {mel_feature.get('error', 'æœªçŸ¥é”™è¯¯')}ï¼›")

    return "\n".join(analysis_lines)

# ====================== 3. ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Šï¼ˆæ›´æ–°å¼‚å¸¸åˆ¤å®šé€»è¾‘ï¼‰ ======================
def generate_reference_report(audio_filename):
    """
    ç”Ÿæˆå®Œæ•´çš„éŸ³é¢‘ä¼ªé€ æ£€æµ‹åˆ†ææŠ¥å‘Šï¼ˆå«ASR+è¯´è¯äºº+ç‰¹å¾å¼‚å¸¸ï¼‰
    æ ¸å¿ƒæ›´æ–°ï¼šåŸºäºLibriSpeechç»Ÿè®¡é˜ˆå€¼åšæ•´ä½“é£é™©è¯„ä¼°
    """
    # æ­¥éª¤1ï¼šè¯»å–ç‰¹å¾æ•°æ®
    feature_result = load_suspicious_features(audio_filename)
    if not feature_result["success"]:
        print(f"âŒ {feature_result['error']}")
        return feature_result
    
    feature_data = feature_result["data"]
    total_segments = feature_data["total_suspicious_segments"]
    extracted_segments = feature_data["extracted_segments_count"]
    audio_path = feature_data["audio_path"]

    # æ­¥éª¤2ï¼šè¯»å–ASR+è¯´è¯äººæ•°æ®
    asr_data = load_asr_diarization_data(audio_filename)

    # æ­¥éª¤3ï¼šæ„å»ºæŠ¥å‘Šå†…å®¹
    report_content = []
    # æŠ¥å‘Šæ ‡é¢˜
    report_content.append(f"# éŸ³é¢‘ä¼ªé€ æ£€æµ‹åˆ†ææŠ¥å‘Š")
    report_content.append(f"## åŸºç¡€ä¿¡æ¯")
    report_content.append(f"- è¯­éŸ³æ–‡ä»¶æ ‡è¯†ï¼š{audio_filename}")
    report_content.append(f"- åŸå§‹éŸ³é¢‘è·¯å¾„ï¼š{audio_path}")
    report_content.append(f"- æ£€æµ‹åˆ°çš„å¯ç–‘ç‰‡æ®µæ€»æ•°ï¼š{total_segments}")
    report_content.append(f"- æˆåŠŸæå–ç‰¹å¾çš„ç‰‡æ®µæ•°ï¼š{extracted_segments}")
    # æ–°å¢ï¼šæ ‡æ³¨é˜ˆå€¼åŸºå‡†æ¥æº
    report_content.append(f"- å¼‚å¸¸åˆ¤å®šåŸºå‡†ï¼šLibriSpeech dev-clean 500æ¡çœŸäººè¯­éŸ³ç»Ÿè®¡ï¼ˆ3ÏƒåŸåˆ™ï¼‰")

    # ===== ASR+è¯´è¯äººåŸºç¡€ä¿¡æ¯ =====
    if asr_data:
        report_content.append(f"- è¯­éŸ³è¯†åˆ«è¯­è¨€ï¼š{asr_data.get('language', 'æœªçŸ¥')}")
        report_content.append(f"- éŸ³é¢‘æ€»æ—¶é•¿ï¼š{asr_data.get('audio_duration', 'æœªçŸ¥')} ç§’")
        report_content.append(f"- è¯†åˆ«æ€»è¯æ•°ï¼š{asr_data.get('total_words', 0)}")
        report_content.append(f"- æ£€æµ‹åˆ°çš„è¯´è¯äººæ•°é‡ï¼š{asr_data.get('total_speakers', 0)}")
        report_content.append(f"- å®Œæ•´è¯­éŸ³å†…å®¹ï¼š{asr_data.get('full_text', 'æ— ')}")
    else:
        report_content.append(f"- è¯­éŸ³è¯†åˆ«çŠ¶æ€ï¼šæœªè·å–åˆ°ASR+è¯´è¯äººæ•°æ®")

    report_content.append(f"\n## å¯ç–‘ç‰‡æ®µç‰¹å¾+è¯­éŸ³å†…å®¹åˆ†æ")

    # æ— å¯ç–‘ç‰‡æ®µåœºæ™¯
    if extracted_segments == 0:
        report_content.append(f"> æœªæ£€æµ‹åˆ°ä»»ä½•å¯ç–‘ç‰‡æ®µï¼Œè¯¥éŸ³é¢‘æ— ä¼ªé€ é£é™©ã€‚")
    # æœ‰å¯ç–‘ç‰‡æ®µåœºæ™¯ï¼šé€ä¸ªåˆ†æï¼ˆå…³è”ASRï¼‰
    else:
        for segment_feature in feature_data["segments_features"]:
            anomaly_analysis = analyze_feature_anomaly(segment_feature, asr_data)
            report_content.append(anomaly_analysis)
            report_content.append("")  # åˆ†æ®µç©ºè¡Œ
        
        # ===== æ ¸å¿ƒæ›´æ–°ï¼šæ•´ä½“é£é™©è¯„ä¼°ï¼ˆåŸºäºLibriSpeeché˜ˆå€¼ï¼‰ =====
        report_content.append(f"\n## æ•´ä½“é£é™©è¯„ä¼°")
        has_anomaly = False
        anomaly_details = []
        
        for seg in feature_data["segments_features"]:
            # MFCCå¼‚å¸¸åˆ¤å®š
            mfcc = seg.get("mfcc_feature", {})
            if mfcc.get("success"):
                mfcc_mean_abs = abs(mfcc["mfcc_stats"]["mean"])
                mfcc_std = mfcc["mfcc_stats"]["std"]
                if mfcc_mean_abs > ANOMALY_THRESHOLDS["mfcc_mean_abs"]:
                    has_anomaly = True
                    anomaly_details.append(f"ç‰‡æ®µ{seg['segment_id']}MFCCå‡å€¼å¼‚å¸¸")
                if mfcc_std > ANOMALY_THRESHOLDS["mfcc_std_upper"]:
                    has_anomaly = True
                    anomaly_details.append(f"ç‰‡æ®µ{seg['segment_id']}MFCCæ ‡å‡†å·®å¼‚å¸¸")
            
            # æ¢…å°”é¢‘è°±å¼‚å¸¸åˆ¤å®š
            mel = seg.get("mel_feature", {})
            if mel.get("success"):
                mel_mean = mel["mel_energy_stats"]["mean"]
                if mel_mean > ANOMALY_THRESHOLDS["mel_energy_upper"] or mel_mean < ANOMALY_THRESHOLDS["mel_energy_lower"]:
                    has_anomaly = True
                    anomaly_details.append(f"ç‰‡æ®µ{seg['segment_id']}æ¢…å°”èƒ½é‡å¼‚å¸¸")
        
        # é£é™©è¯„ä¼°ç»“è®º
        if has_anomaly:
            report_content.append(f"> âš ï¸ æ£€æµ‹åˆ°ä»¥ä¸‹å¼‚å¸¸ï¼š{'; '.join(anomaly_details)}ï¼›è¯¥éŸ³é¢‘**å­˜åœ¨ä¼ªé€ é£é™©**ã€‚")
            if asr_data:
                report_content.append(f"> ğŸ“¢ å¼‚å¸¸ç‰‡æ®µå¯¹åº”çš„è¯­éŸ³å†…å®¹å·²æ ‡æ³¨ï¼Œå¯ç»“åˆè¯­ä¹‰è¿›ä¸€æ­¥éªŒè¯ä¼ªé€ é£é™©ã€‚")
        else:
            report_content.append(f"> âœ… æ‰€æœ‰ç‰‡æ®µç‰¹å¾å‡ç¬¦åˆLibriSpeechçœŸäººè¯­éŸ³åŸºå‡†ï¼Œè¯¥éŸ³é¢‘**ä¼ªé€ é£é™©è¾ƒä½**ã€‚")

    # æ­¥éª¤4ï¼šä¿å­˜æŠ¥å‘Šæ–‡ä»¶ï¼ˆä¿®æ”¹è¿™éƒ¨åˆ†ï¼‰
    report_filename = f"{audio_filename}_fake_detection_report.md"
    # 1. éªŒè¯ç›®å½•æ˜¯å¦å¯å†™
    if not os.path.exists(REFERENCE_OUTPUT_ROOT):
        try:
            os.makedirs(REFERENCE_OUTPUT_ROOT, mode=0o755)  # æ˜¾å¼æŒ‡å®šæƒé™
            print(f"âœ… åˆ›å»ºç›®å½•æˆåŠŸï¼š{REFERENCE_OUTPUT_ROOT}")
        except Exception as e:
            return {
                "success": False,
                "error": f"åˆ›å»ºæŠ¥å‘Šç›®å½•å¤±è´¥ï¼š{str(e)}ï¼ˆæƒé™ä¸è¶³ï¼Ÿï¼‰"
            }
    # 2. ç”Ÿæˆç»å¯¹è·¯å¾„
    report_path = os.path.abspath(os.path.join(REFERENCE_OUTPUT_ROOT, report_filename))
    # 3. å†™å…¥æ–‡ä»¶ï¼ˆæ·»åŠ é”™è¯¯æ•è·ï¼‰
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_content))
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ç”Ÿæˆ
        if os.path.exists(report_path):
            print(f"âœ… MDæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼š{report_path}")
            print(f"âœ… æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(report_path)} å­—èŠ‚")
            return {
                "success": True,
                "audio_filename": audio_filename,
                "report_path": report_path,  # è¿”å›ç»å¯¹è·¯å¾„
                "message": f"åˆ†ææŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆï¼š{report_path}"
            }
        else:
            return {
                "success": False,
                "error": f"æ–‡ä»¶å†™å…¥åä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯æƒé™é—®é¢˜ï¼š{report_path}"
            }
    except PermissionError:
        return {
            "success": False,
            "error": f"æ— å†™å…¥æƒé™ï¼š{report_path}ï¼ˆè¯·æ£€æŸ¥ç›®å½•æƒé™ï¼‰"
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥ï¼š{str(e)}"
        }
# ====================== ä¸»ç¨‹åºå…¥å£ï¼ˆæµ‹è¯•ç”¨ï¼‰ ======================
if __name__ == "__main__":
    test_audio_filename = "LA_E_1000147"  # ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶åï¼ˆå»åç¼€ï¼‰
    result = generate_reference_report(test_audio_filename)
    
    if result["success"]:
        print(f"âœ… {result['message']}")
    else:
        print(f"âŒ {result['error']}")